%
% Kontextfreie Sprachen und pushdown-Automaten
%
\lhead{Kontextfreie Sprachen}
\rhead{Kontextfreie Grammatiken}
\chapter{Stackautomaten und kontextfreie Sprachen\label{chapter-cfl}}
Die regul"aren Sprachen bilden zwar eine interessante Klasse
von Sprachen mit mathematisch sehr attraktiven Eigenschaften,
doch sind viele praktisch wichtige Sprachen nicht regul"ar.
Bereits korrekt geschachtelte Klammern k"onnen nicht mit einem
DEA erkannt werden.

Ursache ist, dass sich ein DEA nicht
daran ``erinnern'' kann, wieviele Klammern schon ge"offnet
worden sind. Um das f"ur beliebig viele Klammern tun zu k"onnen,
br"auchte er unendlich viele Zust"ande (Satz von Myhill-Nerode),
das ist in einem DEA nicht m"oglich. Dazu ist als Erweiterung
eine Art von Speicher n"otig. In diesem Kapitel betrachten
wir Automaten, die mit einem Stack ausgestattet worden sind.

\index{Stack}
\index{Stackautomat}
Auch regul"are Ausdr"ucke sind nicht dazu geeigneet, die Schachtelung
von Klammern auszudr"ucken. Daher brauchen wir auch f"ur die
regul"aren Ausdr"ucke eine Alternative, dies werden die kontextfreien
Grammatiken sein. Es wird sich zeigen, dass die von Stackautomaten
akzeptierten Sprachen auch mit einer kontextfreien Grammatik erzeugt
werden k"onnen, wir haben also eine "ahnliche Situation wie
bei DEAs und regul"aren Ausdr"ucken. So entsteht die neue
Kategorie der kontextfreien Sprachen.
\index{Sprache!kontextfreie}
Nat"urlich werden auch kontextfreie Sprachen nicht alles
abdecken, so dass wir wieder ein Kriterium brauchen, mit dem
man entscheiden kann, ob eine Sprache kontextfrei ist. Wir
werden auch f"ur kontextfreie Sprachen
ein Pumping Lemma finden.

\section{Kontextfreie Sprachen}
\rhead{Kontextfreie Sprachen}
\subsection{Kontextfreie Grammatiken}
Regul"are Sprachen werden dadurch definiert, dass die Zeichen
eines Wortes von links nach rechts gelesen werden, und nach jedem
Zeichen entschieden werden kann, ob das Wort akzeptabel ist.
Der Fokus liegt also auf der Analyse des Strings, und genau dies
macht das "Uberpr"ufen der korrekten Schachtelung von Klammern
schwierig. 

\index{Ausdr\"ucke!arithmetische}
F"ur Klammern ist man sich jedoch meistens eine andere Vorgehensweise
gewohnt. Wenn man arithmetische Ausdr"ucke aufbaut, sagt man zum
Beispiel: ``Wenn man die Summe $a+b$ mit $c$ multiplizeren will,
dann {\bf muss man $a+b$ in Klammern setzen}''.
Klammern werden also immer paarweise gesetzt, und man baut den
Ausdruck sozusagen von innen nach aussen auf.

Wir m"ochten diese Idee ein St"uck weit formalisieren. Das Ziel ist,
W"orter aus den Zeichen {\tt (} und {\tt )} aufzubauen, die korrekte
Klammerausdr"ucke sind. Dazu k"onnen wir bereits bekannte korrekte
Klammerausdr"ucke aneinanderh"angen, oder wir k"onnen einen bereits
bekannten korrekten Ausdruck ``einklammern''. Schreiben wir $A$
f"ur einen Klammerausdruck, bedeutet das, dass wir $A$ gem"ass
folgender Regeln umformen k"onnen:
\begin{align*}
A&\to AA\\
A&\to {\tt (}A{\tt )}
\end{align*}
Dabei ist auf der rechten Seite der ersten Regel gemeint, dass wir zwei beliebige
Ausdr"ucke nehmen k"onnen, sie m"ussen nicht identisch sein. Allerdings
fehlt in diesen Regeln noch ein Anfang, bis jetzt k"onnen wir "uberhaupt
keine W"orter produzieren. Dazu nehmen wir noch eine Regel $A\to\varepsilon$
hinzu, die besagt, dass das leere Wort auch ein akzeptabler
Klammerausdruck ist. Die Regeln
\begin{equation}
\begin{aligned}
A&\to\varepsilon\\
A&\to AA\\
A&\to {\tt (}A{\tt )}
\end{aligned}
\label{regeln-beispiel}
\end{equation}
erzeugen in diesem Sinne alle korrekt geschachtelten Klammerausdr"ucke.

Der Buchstabe $A$ steht offenbar nicht immer f"ur das gleiche, 
er ist Platzhalter f"ur einen korrekten Klammerausdruck. Wir nennen
$A$ eine Variable.
Dagegen stehen die Zeichen {\tt (} und {\tt )} nur f"ur sich selbst, sie
k"onnen nicht weiter ersetzt werden, sie heissen Terminalsymbole.

Damit sind wir bereit f"ur die formale Definition einer kontextfreien
Grammatik.
\begin{definition}
\index{Grammatik!kontextfreie}
Eine kontextfreie Grammatik ist ein Quadrupel $(V,\Sigma,R,S)$ mit
\begin{compactenum}
\index{Variable}
\item $V$ ist eine endliche Menge von Variablen.
\index{Terminalsymbol}
\item $\Sigma$ ist eine endliche Menge von Zeichen, disjunkt zu $V$,
auch genannt die Terminalsymbole.
\item $R$ ist eine Menge von Regeln, eine Regel besteht aus einer
Variable und einer Kette von Variablen und Terminalsymbolen, geschrieben
in der Form $A\to BC{\tt x}$.
\index{Regel!einer kontextfreien Grammatik}
\item $S\in V$ ist die Startvariable.
\index{Startvariable}
\end{compactenum}
\end{definition}
Im Beispiel ist $V=\{A\}$, $\Sigma=\{{\tt (},{\tt )}\}$, $S=A$ und
$R$ enth"alt genau drei Regeln (\ref{regeln-beispiel}).

Zur Abk"urzung erlauben wir, Regeln mit der gleichen Variablen
auf der linken Seite des Pfeils mit einem Vertikalstrich als
Verkn"upfungszeichen auf der rechten Seite zu schreiben, zu lesen
als ``oder'':
\[
A \to \varepsilon\;|\; AA\;|\; {\tt (}A{\tt )}.
\]

Der Name ``kontextfrei'' r"uhrt daher, dass es bei der Anwendung
der Regeln nicht auf den Kontext ankommt, in dem eine Variable
auf der linken Seite des Pfeils~$\rightarrow$ vorkommt. Regeln der
Form 
\[
{\tt a}A\to AA, \qquad {\tt bA}\to BB,
\]
sogenannte kontextsensitive Regeln,
\index{kontextsensitiv}
w"urden dagegen ausdr"ucken, dass die Umwandlung der Variablen $A$
unterschiedlich zu erfolgen hat, wenn ihr verschiedene Zeichen
vorangehen. In diesem Fall k"ame es auf den Kontext an. Solche
Regeln sollen jedoch nicht zugelassen sein.

\subsection{Kontextfreie Sprachen}
Eine Grammatik erzeugt eine Sprache auf die folgende Weise.
Auf die Startvariable werden beliebige Regeln angewendet,
bis keine Variablen mehr vorhanden sind, jetzt steht nur noch
ein Kette von Terminalsymbolen da. Dies wollen wir wie folgt
formalisieren.

\begin{definition}
\index{Regel!erzeugtes Wort}
\index{Ableitung}
Falls $A\to w$ eine Regel einer Grammatik $G$ ist, sagt man
dass die Regel aus $uAv$ den String $uwv$ erzeugt,
geschrieben $uAv\Rightarrow uwv$. Man sagt, $v$ l"asst sich aus
$u$ ableiten, wenn es eine Folge $u_1,\dots,u_n$ gibt mit
\[
u\Rightarrow u_1\Rightarrow u_2\Rightarrow\dots\Rightarrow u_n\Rightarrow v,
\]
auch geschrieben als $u\overset{*}{\Rightarrow} v$
\end{definition}


\begin{definition}
\index{Sprache!von einer CFG erzeugte}
Die Menge aller W"orter, die von einer kontextfreien Grammatik 
$G$ erzeugt werden k"onnen wir mit
\[
L(G)=\{w\in\Sigma^*\;|\; S\overset{*}{\Rightarrow} u\}
\]
bezeichnen.
\end{definition}

\subsection{Beispiele}
\subsubsection{Nat"urliche Zahlen}
Nat"urliche Zahlen sind W"orter "uber dem Alphabet $\Sigma=\{
{\tt 0},
{\tt 1},
{\tt 2},
{\tt 3},
{\tt 4},
{\tt 5},
{\tt 6},
{\tt 7},
{\tt 8},
{\tt 9}\}$, welche von den Grammatikregeln
\begin{align*}
N&\to Z\\
 &\to NZ\\
Z&\to {\tt 0}\;|\;\dots \;|\;{\tt 9}
\end{align*}
erzeugt werden.

Diese Grammatik hat aber den Mangel, dass sie auch Zahlen
mit f"uhrenden Nullen erlaubt. Diese k"onnten dadurch entfernt
werden, dass wir eine weitere Variable $A$ f"ur zul"assige
Anfangsziffern einf"uhren:
\begin{align*}
N&\to A\\
 &\to NZ\\
A&\to {\tt 1}\;|\;\dots\;|\;{\tt 9}\\
Z&\to {\tt 0}\;|\; A
\end{align*}
Leider haben wir damit aber auch die Zahl {\tt 0} verboten,
eine einzelne Ziffer {\tt 0} sollte auch akzeptabel sein.
Wir brauchen also eine zus"atzliche Regel, welche $N\to{\tt 0}$
"uberf"uhrt:
\begin{align*}
N&\to A\\
 &\to NZ\\
 &\to {\tt 0}\\
A&\to {\tt 1}\;|\;\dots\;|\;{\tt 9}\\
Z&\to {\tt 0}\;|\; A
\end{align*}
\subsubsection{Einfache arithmetische Ausdr"ucke}
\index{Ausdr\"ucke!arithmetische}
\index{expression-term-factor Grammatik}
Auch arithemtische Ausdr"ucke k"onnen mit einer
Grammatik erzeugt werden. Da die Zahl der Variablen schnell gr"osser
wird, werden wir im Folgenden auch l"anger Variablennamen zulassen.
Als Alphabet verwenden wir
$\Sigma=\{{\tt 0},\dots,{\tt 9},{\tt +},{\tt *},{\tt (}, {\tt )}\}$.
Die Startvariable dr"uckt aus, was erzeugt werden soll, in diesem Fall
ein Ausdruck, also nennen wir sie {\tt expression}.  Folgende Grammatik
erzeugt die arithmetischen Ausdr"ucke:
\begin{align*}
\text{\tt expression} &\to \text{\tt expression}\;{\tt +}\;\text{\tt term}\\
                      &\to \text{\tt term}\\
\text{\tt term}       &\to \text{\tt term}\;{\tt *}\;\text{\tt factor}\\
                      &\to \text{\tt factor}\\
\text{\tt factor}     &\to {\tt (}\text{\tt expression}{\tt )}\\
                      &\to N\\
N                     &\to Z\\
                      &\to NZ\\
Z                     &\to {\tt 0}\;|\;\dots \;|\;{\tt 9}
\end{align*}

\subsubsection{Eine nicht regul"are Sprache}
Die Sprache $\{{\tt 0}^n{\tt 1}^n\;|\;n\in\mathbb N\}$ wurde bereits
fr"uher als nicht regul"ar erkannt. Sie wird aber von der 
Grammatik $G=(\{S\}, \{{\tt 0},{\tt 1}\}, R, S)$ erzeugt
mit den Regeln
\begin{align*}
S&\rightarrow \varepsilon\\
&\rightarrow {\tt 0}S{\tt 1}
\end{align*}

\subsection{Regul"are Operationen}
In diesem Abschnitt beweisen wir, dass regul"are Sprachen auch
kontextfrei sind. Dazu ist zu zeigen, dass
zu jedem regul"aren Ausdruck eine kontextfreie Grammatik existiert,
die die Sprache erzeugt.

\begin{satz}[Vereinigung]
\index{Vereinigung}
Seien $L_1$ und $L_2$ kontextfreie Sprachen "uber $\Sigma$,
dann ist auch $L_1\cup L_2$ kontextfrei.
\end{satz}

\begin{proof}[Beweis]
Weil $L_i$ kontextfrei sind,  gibt es Grammatiken $G_1$ und $G_2$,
die $L_1$ bzw.~$L_2$ erzeugen. Wir d"urfen sogar annehmen, dass
beide Grammatiken keine
gemeinsamen Variablen haben, dass also $V_1$ und $V_2$ 
disjunkt sind. Wir konstruieren jetzt eine neue Grammatik
$G=(V_1\cup V_2\cup\{S\}, \Sigma, R, S)$ mit einem neuen
Startzustand $S$. Die Regelmenge $R$ besteht einerseits aus
den Regeln in $G_1$ und $G_2$ und andererseits aus der neuen
Regel
\[
S\to S_1 | S_2.
\]
Insgesamt ist also
\[
R=R_1\cup R_2\cup \{S\to S_1, S\to S_2\}.
\]
Die W"orter von $L_1$ werden erzeugt, wenn man $S\to S_1$ als
erste Regel anwendet, die W"orter von $L_2$ jedoch, wenn 
man $S\to S_2$ verwendet. Die Regeln k"onnen nicht ``gemischt''
angewendet werden, weil $V_1\cap V_2=\emptyset$.
\end{proof}

\begin{satz}[Verkettung]
\index{Verkettung}
Sind $L_1$ und $L_2$ kontextfreie Sprachen, dann ist auch $L_1L_2$
kontextfrei.
\end{satz}

\begin{proof}[Beweis]
Seien wieder $G_i$ die Grammatiken, die $L_i$ erzeugen, mit disjunkten
Variablenmengen $V_1\cap V_2=\emptyset$. Dann erzeugt die Grammatik
$G=(V_1\cup V_2\cup\{S\},\Sigma, R,S)$ mit dem neuen Startzustand
und den Regeln
\[
R=R_1\cup R_2\cup \{S\to S_1S_2\}
\]
die Sprache $L_1L_2$.
\end{proof}

\begin{satz}[$*$-Operation]
\index{*-Operation@$*$-Operation}
Ist $L$ kontextfrei, dann auch $L^*$.
\end{satz}

\begin{proof}[Beweis]
Sei $G=(V,\Sigma,R,S)$ die Grammatik, die $L$ erzeugt. Dann erzeugt die
Grammatik
\[
G^*=(V\cup \{S_0\}, \Sigma, R_0, S_0)
\]
mit
\[
R_0=R\cup \{ S_0\to S_0S, S_0\to\varepsilon \}
\]
die Sprache $L^*$.
\end{proof}

\begin{satz} Sei $L$ eine regul"are Sprache, dann gibt es eine
kontextfreie Grammatik $G$ mit $L(G)=L$.
\end{satz}

\begin{proof}[Beweis]
Da regul"are Sprachen mit regul"aren Operationen aus einfacheren
Sprachen aufgebaut werden k"onnen, ist nur zu zeigen, dass die
Konstruktionen, die mit regul"aren Operationen m"oglich sind,
auch durch Produktionsregeln einer Grammatik ausgedr"uckt werden
k"onnen. 
\begin{enumerate}
\item Leere Sprache: Die leere Sprache ist regul"ar. Sie wird auch 
von der Grammatik mit $R=\emptyset$ erzeugt, also ist sie auch
kontextfrei.
\item Ein einzelnes Zeichen: $L=\{{\tt a}\}$. Die Sprache $L$ wird von
der Grammatik $G=(\{S\}, \{{\tt a}\}, \{S\to{\tt a}\}, S)$
erzeugt.
\item Alle Sprachen, die sich aus den eben genannten durch regul"are
Operationen konstruieren lassen, sind ebenfalls kontextfrei. Da
dies f"ur alle regul"aren Sprachen zutrifft folgt, dass alle
regul"aren Sprachen kontextfrei sind.
\end{enumerate}
\end{proof}

Der Satz zeigt, dass die kontextfreien Sprachen eine echte
Obermenge der regul"aren Sprachen sind:
\begin{center}
\includegraphics[width=0.5\hsize]{images/lang-1}
\end{center}
Wir werden sp"ater zeigen, dass kontextfreie Sprachen auch durch
die Existenz eines Stackautomaten charakterisiert werden k"onnen,
der diese Sprachen akzeptiert. Dann wird sich zeigen, dass 
die regul"aren Sprachen diejenigen kontextfreien Sprachen sind,
f"ur deren Analyse der Stack gar nicht ben"otigt wird.

\subsection{Parse Tree}
\index{parse tree}
\index{Ableitungsbaum}
Der Ableitungsbaum eines Wortes einer kontextfreien Sprache
ist eine Darstellung der verwendeten Produktionsregeln in Baum-Struktur.
\index{Ausdr\"ucke!arithmetische}
Die Grammatik f"ur arithmetische Ausdr"ucke produziert zum Beispiel
den Ausdruck {\tt 7 * (3 + 5)}.
Die dabei verwendeten Regeln k"onnen in Baumform wie folgt
dargestellt werden:

{
\tiny
\[
\xymatrix{
	&{\tt expression} \ar[d]
\\
	&{\tt term} \ar[dl] \ar[d] \ar[drrr]
\\
{\tt term}\ar[dddd]
	&{\tt *}\ar[ddddddd]
		&
			&
				&{\tt factor}\ar@/_40pt/[dddddddll]\ar[d]\ar@/^30pt/[dddddddrr]
					&
						&
\\
	&
		&
			&
				&{\tt expression}\ar[dl]\ar[dddddd]\ar[ddr]
					&
\\
	&
		&
			&{\tt expression}\ar[d]
				&
					&
						&
\\
	&
		&
			&{\tt term}\ar[d]
				&
					&{\tt term}\ar[d]
						&
\\
{\tt factor}\ar[d]
	&
		&
			&{\tt factor}\ar[d]
				&
					&{\tt factor}\ar[d]
						&
\\
N\ar[d]
	&
		&
			&N\ar[d]
				&
					&N\ar[d]
						&
\\
Z\ar[d]
	&
		&
			&Z\ar[d]
				&
					&Z\ar[d]
						&
\\
{\tt 7}
	&{\tt *}
		&{\tt (}
			&{\tt 3}
				&{\tt +}
					&{\tt 5}
						&{\tt )}
}
\]
}

Offenbar spielt es keine Rolle, ob erst die Regelanwendungen
im linken Teil des Baumes geschehen, oder die Regelanwendungen im
rechten Teil. Zwei Ableitungen k"onnen daher als gleich angesehen
werden, wenn sich die Ableitungsb"aume nicht unterscheiden.

\begin{definition}
\index{Aquivalenz@\"Aquivalenz!von Ableitungen}
Zwei Ableitungen eines Wortes $w$ einer kontextfreien Sprache $L(G)$
heissen "aquivalent, wenn sie den gleichen Ableitungsbaum haben.
Hat eine Sprache W"orter mit verschiedenen Ableitungen, heisst
sie mehrdeutig (engl.~ambiguous).
\end{definition}

Von besonderem praktischem Interesse sind Grammatiken, in denen
Ableitungen immer eindeutig sind. Die
\index{expression-term-factor Grammatik}
{\tt expression}-{\tt term}-{\tt factor}-Grammatik f"ur einfache
arithmetische Ausdr"ucke erf"ullt diese Bedingung. 
Ein Beispiel f"ur eine zweideutige Grammatik ist
\[
G=(\{S\}, \{0,1\}, \{S\to 0S1|1S0|SS|\varepsilon\}, S).
\]
$G$ erzeugt die Sprache
$\{w\in \{0,1\}^*\,|\, |w|_0 = |w|_1\}$, aber das Wort
$001011$ hat mindestens zwei verschiedene Ableitungen:
\[
\xymatrix{
&&&S\ar@/_10pt/[dddddlll]\ar[d]\ar@/^30pt/[dddddrrrr]
\\
&&&S\ar[dl]\ar[drr]
\\
 &&S\ar[dddl]\ar[d]\ar[dddr]&&&S\ar[dddl]\ar[d]\ar[dddr]&&
\\
&&S\ar[d]&&&S\ar[d]&&
\\
 & &\varepsilon& & &\varepsilon& & 
\\
0&0&           &1&0&           &1&1
}
\]
oder
\[
\xymatrix{
&&&S\ar@/_/[dddddlll]\ar[d]\ar@/^/[dddddrrr]
\\
&&&S\ar@/_/[ddddll]\ar[d]\ar@/^/[ddddrr]&&&
\\
&&&S\ar@/_/[dddl]\ar[d]\ar@/^/[dddr]&&&
\\
&&&S\ar[d]&&&
\\
&&&\varepsilon&&&
\\
0&0&1&&0&1&0
}
\]


\subsection{Chomsky Normalform}
Bei den regul"aren Sprachen war der Algorithmus zur Reduktion
auf den minimalen Automaten die Voraussetzung, Automaten vergleichen
zu k"onnen. Eine "ahnliche Normalform w"unscht man sich auch f"ur
Grammatiken, und definiert daher:
\begin{definition}
\index{Chomsky Normalform}
Eine kontextfreie Grammatik  ist in Chomsky Normalform, wenn
jede Regel von der Form
\begin{align*}
A&\to BC\qquad\text{oder}\\
A&\to a
\end{align*}
ist, wobei $A\in V$, $a\in\Sigma$ und $B, C\in V\setminus\{S\}$.
Zus"atzlich ist die Regel $S\to\varepsilon$ erlaubt. 
\end{definition}

Bei einer Grammatik in Chomsky Normalform kann man sofort absch"atzen,
wieviele Regelanwendungen notwendig sind, um ein gegebenes Wort
zu erzeugen. Wendet man eine der Regeln $A\to BC$ and, wird das
Wort verl"angert. Dies kann nicht mehr r"uckg"angig gemacht
werden, weil dazu eine Regel notwendig w"are, die eine Variable
in das leere Wort umwandelt. Die Regel $S\to\varepsilon$ ist aber
die einzige erlaubte derartige Regel, aber sie kann nie angewendet
werden weil $B$ und $C$ nicht $S$ sein k"onnen. Es werden also
h"ochstens $|w|-1$ Anwendungen von Regeln $A\to BC$ ben"otig, und dann
nochmals $|w|$ Anwendungen von Regeln $A\to a$, welche die Variablen
in Terminalsymbole umwandeln. 

Eine Grammatik kann aus folgenden Gr"unden nicht der Chomsky Normalform
entsprechen:
\begin{enumerate}
\item Die Startvariable $S$ k"onnte auf der rechten Seite einer
Regel vorkommen.
\item Eine $\varepsilon$-Regel $A\to\varepsilon$ mit einer Variablen
$A$, die nicht die Startvariable ist.
\index{unit rule}
\item Regeln der Form $A\to B$, sogenannte unit rules.
\item Regeln der Form $A\to u_1u_2\dots u_n$, mit $n>2$, wobei
die $u_i$ sowohl andere Variablen wie auch Terminalsymbole sein 
k"onnen.
\end{enumerate}
Es ist m"oglich, alle diese Defekte zu ``reparieren'', und jede
Grammatik in Chomsky Normalform "uberzuf"uhren, wie der folgende Satz
zeigt.

\begin{satz}
Eine kontextfreie Sprache wird erzeugt von einer Grammatik
in Chomsky Normalform.
\end{satz}

\begin{proof}[Beweis]
Ein kontextfreie Sprache $L$ wird von einer kontextfreien Grammatik
$G$ akzeptiert. Es ist zu zeigen, dass diese Grammatik in
"aquivalente Chomsky Normalform gebracht werden kann.
Weiter oben haben wir eine Liste
m"oglicher Defekte zusammengestellt, die eine Grammatik davon abhalten,
Chomsky Normalform zu haben. Um Chomsky Normalform zu erreichen,
m"ussen also genau diese Defekte behoben werden.
Dazu sind folgende Schritte notwendig
\begin{enumerate}
\item Damit die Startvariable nicht mehr auf der rechten Seite auftreten
kann, wird eine neue Startvariable $S_0$ hinzugef"ugt, sowie eine
neue Regel
$S_0\to S$, wobei $S$ die urspr"ungliche Startvariable war.
\item Entfernung aller $\varepsilon$-Regeln. Solche Regeln erlauben,
eine Variable ``wegzulassen''. F"ur jede Regel, die auf der rechten
Seite eine solche Variable enth"alt, f"ugen wir eine zus"atzliche
Regel hinzu, bei der die $\varepsilon$-Regel auf der rechten Seite
angwendet worden ist, d.\,h.~die Variable auf der rechten Seite weggelassen
wurde.  Aus
\[
\left.
\begin{aligned}
A&\to\varepsilon\\
B&\to AC\\
\end{aligned}
\right\}
\qquad
\Rightarrow
\qquad
\left\{
\begin{aligned}
B&\to AC\\
&\to C\\
\end{aligned}
\right.
\]
Die $\varepsilon$-Regeln werden damit unn"otig, mit der einzig
m"oglichen Ausnahme einer Regel $S_0\to\varepsilon$, die nicht
eliminiert werden kann, weil $S_0$ nicht auf der rechten Seite
vorkommt.
\item Entfernung von sogenannten ``unit rules'': Regeln mit einer einzelnen Variablen auf der rechten Seite, also 
der Form $A\to B$ k"onnen dadurch eliminert werden, dass man
sie in jeder Regel anwendet, die $B$ auf der linken Seite enth"alt.
\[
\left.
\begin{aligned}
A&\to B\\
B&\to CD
\end{aligned}
\right\}
\qquad\Rightarrow\qquad
\left\{
\begin{aligned}
A&\to CD\\
B&\to CD
\end{aligned}
\right.
\]
Wieder muss man die urspr"ungliche Regel $B\to CD$ nat"urlich
behalten, denn $B$ k"onnte ja auch auf andere Art erhalten worden
sein als mit der Regel $A\to B$.
\item Verkettungen: Eine Regel der Form $A\to u_1\dots u_n$ kann mit
Hilfe neuer Variablen $A_1,\dots,A_{n-2}$ in folgende Regeln abgebildet
werden
\begin{align*}
A&\to u_1A_1\\
A_1&\to u_2A_2\\
A_2&\to u_3A_3\\
&\vdots\\
A_{n-2}&\to u_{n-1}u_n
\end{align*}
Falls $u_i$ ein Terminalsymbol ist, ersetzen wir $u_i$ in obigen
Regeln durch eine neue Variable $U_i$ und f"ugen die Regel
$U_i\to u_i$ hinzu. Damit erhalten alle hinzugef"ugten Regeln
Chomsky Normalform.
\end{enumerate}
Die so konstruierte Grammatik hat Chomsky Normalform und erzeugt die
gleiche Sprache.
\end{proof}

\subsubsection{Beispiel}
Man bringe die Grammatik "uber dem Alphabet $\Sigma=\{{\tt a},{\tt b}\}$
mit den Regeln
\begin{align*}
S&\to ASA\;|\; {\tt a}B\\
A&\to B\;|\; S\\
B&\to{\tt b}\;|\; \varepsilon
\end{align*}
in Chomsky Normalform.

\begin{enumerate}
\item Neue Startvariable hinzuf"ugen:
\begin{align*}
S_0&\to S\\
S&\to ASA\;|\; {\tt a}B\\
A&\to B\;|\; S\\
B&\to{\tt b}\;|\; \varepsilon
\end{align*}
\item $\varepsilon$-"Uberg"ange entfernen: Die einzige $\varepsilon$-Regel
ist $B\to\varepsilon$, d.\,h.~zu jeder Regel mit $B$ auf der rechten
Seite gibt es auch eine Regel, in welcher auf der rechten Seite das $B$
weggelassen wurde:
\begin{align*}
S_0&\to S\\
S&\to ASA\;|\; {\tt a}B\;|\;{\tt a}\\
A&\to B \;|\; \varepsilon \;|\; S\\
B&\to{\tt b}
\end{align*}
In diesem Schritt ist eine neue $\varepsilon$-Regel entstanden, die
man auch noch gleichartig behandeln muss:
\begin{align*}
S_0&\to S\\
S&\to ASA \;|\; AS \;|\; SA \;|\; {\tt a}B\;|\;{\tt a}\\
A&\to B \;|\; S\\
B&\to{\tt b}
\end{align*}
\item Einheits-Regeln sind $A\to B$, $A\to S$ und $S_0\to S$,
die alle angewendet werden
m"ussen. Zuerst $A\to B$:
\begin{align*}
S_0&\to S\\
S&\to ASA \;|\; AS \;|\; SA \;|\; {\tt a}B\;|\;{\tt a}\\
A&\to S\\
B&\to{\tt b}\\
A&\to{\tt b}
\end{align*}
dann $A\to S$
\begin{align*}
S_0&\to S\\
S&\to ASA \;|\; AS \;|\; SA \;|\; {\tt a}B\;|\;{\tt a}\\
A&\to ASA \;|\; AS \;|\; SA \;|\; {\tt a}B\;|\;{\tt a}\;|\;{\tt b}\\
B&\to{\tt b}
\end{align*}
und zum Schluss $S_0\to S$
\begin{align*}
S_0&\to ASA \;|\; AS \;|\; SA \;|\; {\tt a}B\;|\;{\tt a}\\
S&\to ASA \;|\; AS \;|\; SA \;|\; {\tt a}B\;|\;{\tt a}\\
A&\to ASA \;|\; AS \;|\; SA \;|\; {\tt a}B\;|\;{\tt a}\;|\;{\tt b}\\
B&\to{\tt b}
\end{align*}
Wobei wir die von $A$ ausgehenden Regeln zusammenfassen k"onnen:
\begin{align*}
S_0&\to ASA \;|\; AS \;|\; SA \;|\; {\tt a}B\;|\;{\tt a}\\
S&\to ASA \;|\; AS \;|\; SA \;|\; {\tt a}B\;|\;{\tt a}\\
A&\to ASA \;|\; AS \;|\; SA \;|\; {\tt a}B\;|\;{\tt a}\;|\; {\tt b}\\
B&\to{\tt b}
\end{align*}
\item L"angere Verkettungen gibt es nur bei $S\to ASA$, wir f"uhren
also eine zus"atzliche Variable $A_1$ und ersetzen $S\to ASA$ durch
die Regeln $S\to AA_1$ und $A_1\to SA$:
\begin{align*}
S_0&\to AA_1 \;|\; AS \;|\; SA \;|\; {\tt a}B\;|\;{\tt a}\\
S&\to AA_1 \;|\; AS \;|\; SA \;|\; {\tt a}B\;|\;{\tt a}\\
A&\to AA_1 \;|\; AS \;|\; SA \;|\; {\tt a}B\;|\;{\tt a} \;|\; {\tt b}\\
A_1&\to SA\\
B&\to{\tt b}\\
\end{align*}
Bleiben nur noch die Regeln $A\to{\tt a}B$ und $S\to{\tt a}B$. Dazu
wird eine weitere Variable $U$ die zusammen mit der Regel $U\to{\tt a}$
das Terminalsymbol {\tt a} in den genannten Regeln ersetzen kann:
\begin{align*}
S_0&\to AA_1 \;|\; AS \;|\; SA \;|\; UB\;|\;{\tt a}\\
S&\to AA_1 \;|\; AS \;|\; SA \;|\; UB\;|\;{\tt a}\\
A&\to AA_1 \;|\; AS \;|\; SA \;|\; UB\;|\;{\tt a} \;|\; {\tt b}\\
A_1&\to SA\\
U&\to {\tt a}\\
B&\to{\tt b}\\
\end{align*}
Damit ist die Grammatik in Chomsky Normalform gefunden.
\end{enumerate}

\begin{satz}
Ist $L$ eine kontextfreie Sprache, dann gibt es eine Grammatik,
mit der jedes Wort $w$ mit einem Ableitungsbaum der Tiefe h"ochstens
$2|w|-1$ abgeleitet werden kann.
\end{satz}

\begin{proof}[Beweis]
Es gibt eine Grammatik in Chomsky Normalform, mit der die W"orter
der Sprache $L$ erzeugt werden k"onnen.
Da in Chomsky Normalform nur Regeln der Form $A\to BC$ und $A\to a$
enthalten kann, braucht es genau $|w|-1$ Anwendungen von Regeln der Form
$A\to BC$, um aus der Startvariable einen String aus $|w|$ Zeichen
herzustellen. Weitere $|w|$ Anwendungen von Regeln der Form $A\to a$
wandeln Variablen in Terminalsymbole um. Somit kann ein Wort mit
genau $2|w|-1$ Regelanwendungen produziert werden.
\end{proof}


\subsection{Ein deterministischer Parse-Algorithmus}
Ob ein Wort zu einer regul"aren Sprache geh"ort, kann in linearer
Zeit entschieden werden. Dazu erzeugt man einen DEA, der die Sprache
akzeptiert, und testet ein Wort damit.

F"ur kontextfreie Sprachen gibt es ebenfalls einen deterministischen
Algorithmus, den Cocke-Younger-Kasami Algorithmus. Dazu verwendet
man die Grammatik in Chomsky-Normalform. Der Algorithmus arbeitet
rekursiv um die Frage zu beantworten, ob ein Wort $w$ aus einer
Variablen $A$ der Grammatik $G=(V,\Sigma,R,S)$  abgeleitet
werden kann.

Um das leere Wort zu erzeugen, muss die Regel $S\to\varepsilon$
angewendet werden, falls sie vorhanden ist.

Ein Wort $w$ mit L"ange $|w|=1$ kann genau dann aus der Variablen 
$A$ abgeleitet werden wenn es eine Regel $S\to a$ gibt. 
Es kann h"ochstens $|\Sigma|$ solche Regeln geben.

Um ein Wort der L"ange $|w|>1$ zu erzeugen, muss mindestens eine
Regel der Form
$A\to BC$ verwendet werden. Sie teilt das Wort in in zwei Teile
$w_1$ und $w_2$ mit je geringerer L"ange: $w=w_1w_2$ und 
$|w_i|<|w|$. Es gibt $|w|-1$ solche Zerlegungen. Um also zu testen,
ob die Grammatik das Wort $w$ aus der Variablen $A$ ableitet,
muss man also rekursiv alle m"oglichen
Zerlegungen des Wortes daraufhin testen, ob sie von den Variablen
$B$ bzw.~$C$ erzeugt werden, ob also $B\overset{*}\Rightarrow w_1$
und $C\overset{*}\Rightarrow w_2$ gilt, und dies f"ur jede
Regel $A\to BC\in R$. Es gibt weniger als $|R|$ solche Regeln.

Zur Implementation dieses rekursiven Algorithmus braucht man
also eine Funktion {\tt ableitbar}, die die Frage beantwortet,
ob ein Wort aus einer bestimmten Variable ableitbar ist. Sie
ruft sich f"ur jede Aufteilung des Wortes in zwei Teile
$w_1$ und $w_2$ und f"ur jede Regel $A\to BC$ selbst auf, um
herauszufinden, ob $w_1$ aus $B$ und $w_2$ aus $C$ ableitbar ist.

Der Pseudocode Algorithm~\ref{cyk-algorithm-code} implementiert
diesen Algorithmus.
\begin{algorithm}
\begin{algorithmic}[1]
\STATE {\tt boolean ableitbar(}Variable $V$, Wort $w${\tt ) \{}
\STATE \hskip1em{\tt if ((}$|w| = 0${\tt )} und {\tt (}Regel $S\to\varepsilon$ vorhanden{\tt )) \{}
\STATE \hskip2em {\tt return true; }
\STATE \hskip1em{\tt \}}
\STATE \hskip1em{\tt if ((}$|w| = 1${\tt )} und {\tt (}Regel $V\to w$ vorhanden{\tt ) \{}
\STATE \hskip2em{\tt return true; }
\STATE \hskip1em{\tt \}}
\STATE \hskip1emF"ur jede Unterteilung $w=w_1w_2$ mit $|w_1| > 0$ und $|w_2| > 0$ {\tt \{}
\STATE \hskip2emF"ur jede Regel $V\to AB$ {\tt \{}
\STATE \hskip3em{\tt if ((ableitbar(}$A,w_1${\tt ) \&\& (ableitbar(}$B,w_2${\tt )) \{ }
\STATE \hskip4em{\tt return true;}
\STATE \hskip3em{\tt \}}
\STATE \hskip2em{\tt \}}
\STATE \hskip1em{\tt \}}
\STATE \hskip1em{\tt return false;}
\STATE {\tt \}}
\end{algorithmic}
\caption{Algorithmus von Cocke-Younger-Kasami\label{cyk-algorithm-code}}
\end{algorithm}


\begin{satz}[Cocke-Younger-Kasami]
\index{Algorithmus!Cocke-Younger-Kasami}
\index{Algorithmus!CYK|see{Cocke-Younger-Kasami}}
\index{Cocke-Younger-Kasami}
\label{cyk-algorithm}
Es gibt einen deterministischen Algorithmus mit Komplexit"at
$O(|w|^3)$, welcher entscheidet, ob $w\in L(G)$.
\end{satz}

Das Argument weiter oben erkl"art, dass es einen determinisitischen
Algorithmus gibt. Es erkl"art aber nicht, warum die Komplexit"at
nur $O(|w|^3)$ ist. "Ahnlich wie man bei einer rekursiven Berechnung
der Fibonacci-Zahlen die Komplexit"at von $O(2^n)$ auf $O(n)$ reduzieren
kann, indem man bereits berechnet Fibonacci-Zahlen speichert, kann
man auch beim obigen rekursiven Algorithmus die Komplexit"at wesentlich
reduzieren, indem man bereits gepr"ufte Ableitungen zwischenspeichert.
Wir verzichten jedoch auf eine detaillierte Durchf"uhrung dieses
Beweises.

\section{Stackautomaten}
\index{Stackautomat}
\rhead{Stackautomaten}
Kontextfreie Grammatiken erzeugen Sprachen, die von einem DEA nicht
akzeptiert werden k"onnen. Es ist also eine erweiterte Maschine
n"otig, wenn sie solche Sprachen erkennen soll. Die Erweiterung muss
den DEA mit einem Speicher ausstatten, der zum Beispiel erlaubt, 
in der Sprache $\{0^n1^n\,\;|\; n\in\mathbb N\}$ die Anzahl der
$0$ zu speichern, damit sp"ater die Anzahl der $1$
damit verglichen werden kann.

Die einfachste Art von Speicher f"ur diesen Zweck ist ein Stack:
man legt die $0$ auf den Stack, und entfernt f"ur jede $1$ eine 
$0$. Wenn der Stack am Schluss leer ist, es ``aufgeht'', ist das
Wort akzeptabel.

\subsection{Formale Definition}
\begin{definition}
Ist $\Sigma$ eine endliche Menge, die das leere Wort $\varepsilon$
nicht enth"alt, dann setzen wir 
\[
\Sigma_\varepsilon = \Sigma\cup \{\varepsilon\}.
\]
\end{definition}

\begin{definition}
\index{Stackautomat}
\index{Pushdown-Automat|see{Stackautomat}}
Ein Stackautomat ist ein $6$-Tupel $(Q,\Sigma,\Gamma,\delta,q_0,F)$
mit endlichen Mengen $Q$, $\Sigma$, $\Gamma$ und $F$ und folgenden
Bezeichungen und Einschr"ankungen
\begin{enumerate}
\index{Zustand}
\item Die Elemente von $Q$ heissen Zust"ande.
\index{Eingabe-Alphabet}
\item $\Sigma$ ist das Eingabe-Alphabet
\index{Stack-Alphabet}
\item $\Gamma$ ist das Stack-Alphabet
\item $\delta\colon Q\times \Sigma_\varepsilon\times\Gamma_\varepsilon
\to P(Q\times\Gamma_\varepsilon)$ heisst "Ubergangsfunktion
\index{Startzustand}
\item $q_0\in Q$ heisst Startzustand
\index{Akzeptierzustand}
\item $F\subset Q$ heisst Menge der Akzeptierzust"ande.
\end{enumerate}
\end{definition}
Die Elemente $Q$, $q_0$ und $F$ scheinen f"ur sich genommen
einen endlichen Automaten zu definieren, jedoch einen, der je nach
dem Wert, der als drittes Argument der "Ubergangsfunktion
"ubergeben wird, sich etwas anders verh"alt. Die Idee ist, dass
dieses dritte Argument von einem Stack gelesen werden soll, der in
der Definition nicht explizit ausgedr"uckt sein muss, da er
sich f"ur alle Stackautomaten gleich verh"alt: Man kann Zeichen
aus $\Gamma$ dort hinschreiben, oder von dort lesen\footnote{Man k"onnte
die Definition eines Stackautomaten auch als C++-Template mit
sechs Template-Argumenten betrachten. Um den Stack zu
implementieren, muss man nur wissen, was man darauf ablegen will,
man muss also nur $\Gamma$ kennen.}.

Im Gegensatz zu den endlichen Automaten, wo wir grossen Wert auf die
Unterscheidung zwischen deterministischen und nicht deterministischen
Automaten gelegt haben, definieren wir Stackautomaten nur
``nichtdeterministisch''.

\subsection{Gerichteter beschrifteter Graph}
% XXX Idee: Zu den Stack-Operationen Bilder hinzufuegen
\index{Graph!gerichteter!beschrifteter!eines Stackautomaten}
Auch zu einem Stackautomaten gibt es einen gerichteten beschrifteten
Graphen. Die "Ubergangsfunktion legt jetzt zu jedem Zustand 
fest, was f"ur ein Zeichen verarbeitet wird, was f"ur ein
neuer Zustand erreicht wird, und wie sich der Stackinhalt
ver"andert.
Dazu wird diese zus"atzliche Information der Beschriftung der Pfeile
hinzugef"ugt.
\[
\xymatrix{
*++[o][F]{p}\ar[r]^{a,b\to c}
	&*++[o][F]{q}
}
\]
bedeutet, dass der Automat bei der Verarbeitung eines Zeichens $a$
vom Zustand $p$ in den Zustand $q$ "ubergeht, wenn gleichzeitig
ein Zeichen $b$ zuoberst auf dem Stack durch das Zeichen $c$
ersetzt werden kann. F"ur "Uberg"ange mit dem leeren Wort gilt:
\[
\xymatrix{
*++[o][F]{p}\ar[r]^{a,\varepsilon\to c}
	&*++[o][F]{q}
		&\text{$c$ wird auf den Stack gelegt}
\\
*++[o][F]{p}\ar[r]^{a,b\to\varepsilon}
	&*++[o][F]{q}
		&\text{$b$ wird vom Stack entfernt}
\\
*++[o][F]{p}\ar[r]^{a,\varepsilon\to\varepsilon}
	&*++[o][F]{q}
		&\text{Stack bleibt unver"andert}
}
\]
In allen F"allen darf $a$ auch das leere Wort sein, was Operationen
ergibt, die den Stack ver"andern, ohne dass dazu Input verarbeitet werden
muss.

% XXX weitere Illustrationen der Stack-Operationen hinzufuegen:
% p --- eps,b->eps ---> q
% p --- eps,eps->eps ---> q

\subsection{Beispiele}
\subsubsection{Stackautomaten k"onnen ``z"ahlen''}
Um die W"orter der nicht regul"are Sprache $L=\{0^n1^n\;|\;n\in \mathbb N\}$
zu erkennen, braucht man einen Z"ahler, mit dem man die Zahl der $0$
mit der Zahl der $1$ vergleichen kann. In einem endlichen Automaten
gab es daf"ur keinen Platz, aber der Stack eines Stackautomaten kann
diese Aufgabe "ubernehmen.

Um zu erkennen, dass der Stack wieder leer ist, braucht man ein Zeichen,
welches den Anfang des Stack markiert. Das Stack-Alphabet muss also 
etwas gr"osser sein, wir f"ugen das Zeichen {\tt \$} hinzu, also
$\Gamma=\{{\tt 0},{\tt 1},{\tt \$}\}$.
Die Idee des konstruierten Stackautomaten ist den Stack als Ablage der
verarbeiteten {\tt 0} zu verwenden, und anschliessend beim verarbeiten
der  {\tt 1} die Nullen wieder vom Stack zu nehmen. Wenn alles ``aufgeht'',
wurde ein Wort mit gleich vielen Nullen wie Einsen verarbeitet, also 
ein Wort in $L$.
\[
\entrymodifiers={++[o][F]}
\xymatrix{
*+\txt{}\ar[r]
	&{}\ar[r]^{\varepsilon,\varepsilon\to{\tt \$}}
		&{} \ar@(ur,dr)^{{\tt 0},\varepsilon\to{\tt 0}}
		    \ar[d]^{\varepsilon,\varepsilon\to\varepsilon}
\\
*+\txt{}
	&*++[o][F=]{}
		&{}\ar[l]^{\varepsilon,{\tt \$}\to\varepsilon}
		   \ar@(ur,dr)^{{\tt 1},{\tt 0}\to\varepsilon}
}
\]
Damit ist gezeigt, dass die nicht regul"are Sprache $L$ von einem Stackautomaten
akzeptiert wird.

\subsubsection{Die Sprache $L=\{a^ib^jc^k\;|\;i,j,k\in\mathbb N, i=j\vee i=k\}$}
Auch in dieser Sprache muss man die $a$ auf den Stack legen, um sie
zu z"ahlen. Dann muss man sich nicht deterministisch entscheiden,
ob man auf gleich viele $b$ oder auf gleich viele $c$ testen will,
beides kann man nicht haben, da nach einem Test der Stack wieder leer
ist. Als Alphabet verwenden wir daher wieder $\Gamma=\{a,b,c,{\tt\$}\}$.
Der Automat muss also zun"achst das Zeichen {\tt\$} auf den Stack legen.
Dann beginnt er entweder, die $b$ zu z"ahlen, und die $c$ zu ignorieren,
oder die $b$ zu ignorieren und dann die $c$ zu z"ahlen.
\[
\entrymodifiers={++[o][F]}
\xymatrix{
*+\txt{}
	&*+\txt{}\ar[d]
\\
*+\txt{}
	&{}\ar[d]^{\varepsilon,\varepsilon\to{\tt\$}}
\\
{}\ar@(u,l)_{b,a\to\varepsilon}
  \ar[d]_{\varepsilon,\varepsilon\to\varepsilon}
	&{}\ar@(dl,dr)_{a,\varepsilon\to a}
	   \ar[l]_{\varepsilon,\varepsilon\to\varepsilon}
	   \ar[r]^{\varepsilon,\varepsilon\to\varepsilon}
		&{}\ar@(u,r)^{b,\varepsilon\to\varepsilon}
		   \ar[d]^{\varepsilon,\varepsilon\to\varepsilon}
\\
{}\ar@(l,d)_{c,\varepsilon\to\varepsilon}
  \ar[r]_{\varepsilon,{\tt\$}\to\varepsilon}
	&*++[o][F=]{}
		&\ar@(r,d)^{c,a\to\varepsilon}
		  \ar[l]^{\varepsilon,{\tt\$}\to\varepsilon}
}
\]

\subsection{"Aquivalenz von Stackautomaten und CFG}
So wie regul"are Ausdr"ucke und endliche Automaten zur Beschreibung
von regul"aren Sprachen "aquivalent sind, so sind auch kontexfreie Grammatiken
mit Stackautomaten "aquivalent:

\begin{satz}
Eine Sprache ist genau dann kontextfrei, wenn sie von einem
Stackautomaten akzeptiert wird.
\end{satz}

Es ist einerseits zu zeigen, dass sich f"ur jede Grammatik ein Stackautomat
finden l"asst, der die von der Grammatik erzeugte Sprache akzeptiert.
Andererseits muss auch zu einem beliebigen Stackautomaten eine 
Grammatik gefunden werden, welche die gleiche Sprache erzeugt, die
auch der Stackautomat akzeptiert.

\begin{hilfssatz}\label{hilfssatz_cfg_to_pushdown}
Ist $L$ eine kontextfreie Sprache mit Grammatik $G=(V,\Sigma,R,S)$,
dann gibt es einen Stackautomaten, der $L$ akzeptiert.
\end{hilfssatz}

\begin{proof}[Beweis]
Die Grammatik erzeugt die W"orter dadurch, dass sie auf die Startvariable
immer wieder Regeln aus $R$ anwendet. Diesen Prozess kann man auf dem
Stack nachbilden. Zu Beginn legt man die Startvariable auf den Stack.
Die Anwendung von Regeln besteht darin, eine Variable vom Stack zu nehmen,
und stattdessen einen die Zeichen auf der rechten Seite der Regel
auf den Stack zu legen. 

Als Stackalphabet verwenden wir $\Gamma = V\cup \Sigma \cup \{{\tt \$}\}$.
Zu Beginn wird das Symbol {\tt\$} auf den Stack gelegt, am Schluss wird 
es wieder entfernt, es sind also insgesamt vier Zust"ande erforderlich:
\[
\entrymodifiers={++[o][F]}
\xymatrix{
*+\txt{}\ar[r]
	&{q_0} \ar[r]^{\varepsilon,\varepsilon\to{\tt\$}}
		&{S}\ar[r]^{\varepsilon,\varepsilon\to S}
			&{R}\ar[r]^{\varepsilon,{\tt\$}\to\varepsilon}
				&*++[o][F=]{A}
}
\]
Jetzt m"ussen im Zustand $R$ noch "Uberg"ange hinzugef"ugt werden, mit denen
die Regeln abgebildet werden.
Dazu d"urfen wir annehmen, dass die Grammatik bereits in Chomsky Normalform
ist. Wir m"ussen also nur noch f"ur Regeln der Form $A\to BC$ und $A\to a$
in "Uberg"ange abbilden. Die Regel $A\to a$ ersetzt die Variable $A$
durch das Terminalsymbol $a$, das erreicht man mit dem "Ubergang
\[
\entrymodifiers={++[o][F]}
\xymatrix{
{R}\ar@(ur,dr)^{\varepsilon,A\to a}
}
\]
F"ur die Regel $A\to BC$ brauchen wir einen zus"atzlichen Zustand mit
den beiden "Uberg"angen
\[
\entrymodifiers={++[o][F]}
\xymatrix{
R\ar@/^/[r]^{\varepsilon,A\to C}
	&{}\ar@/^/[l]^{\varepsilon,\varepsilon\to B}
}
\]
Sobald auf dem Stack ein Terminalsymbol liegt, kann dieses mit
den Regeln der Grammatik nicht mehr verarbeitet werden, muss 
aber auf das n"achste Zeichen des Eingabewortes passen. Die Regel
\[
\entrymodifiers={++[o][F]}
\xymatrix{
{R}\ar@(ur,dr)^{a,a\to\varepsilon}
}
\]
entfernt ein solches Terminalssymbol, wenn das gleiche Symbol
als Input anliegt.

Der so konstruierte Automat akzeptiert genau die
W"orter, die von der Grammatik erzeugt werden.
\end{proof}

Wir k"onnten diesen Beweis mit folgender Notation noch etwas
pr"agnanter formulieren. Die Regeln $A\to BC$ werden dadurch abgebildet,
dass ein zus"atzlicher Zustand hinzugef"ugt wird, um in zwei Schritten
die Variable $A$ vom Stack zu entfernen und stattdessen die zwei Variablen
$BC$ dort abzulegen. Man m"ochte also eigentlich die Operatione
$\varepsilon,A\to BC$ implementieren. Daher soll das Symbol
\[
\entrymodifiers={++[o][F]}
\xymatrix{
{p}\ar[d]^{a,s\to xyz}
\\
{q}
}
\]
implizit zwei neue Zust"ande $q_1$ und $q_2$ und die zugh"origen
"Uberg"ange wie im folgenden Zustandsdiagramm definieren:
\[
\entrymodifiers={++[o][F]}
\xymatrix{
{p}\ar[r]^{a,s\to z}
	&{q_1}\ar[d]^{\varepsilon,\varepsilon\to y}
\\
{q}
	&{q_2}\ar[l]^{\varepsilon,\varepsilon\to x}
}
\]
Analog f"ur beliebige W"orter auf der rechten Seite der Regel,
f"ur ein Wort $w$ der L"ange $|w|$ m"ussen $|w|-1$ neue
Zust"ande hinzugef"ugt werden.

\begin{beispiel}[\bf Beispiel]
F"ur die Grammatik mit den Regeln $S\to {\tt 0}S{\tt 1}\;|\;\varepsilon$
finde man einen Stackautomaten.

Wir haben zwar nur die "Ubersetzung f"ur Regeln einer Grammatik in
Chomsky Normalform diskutiert, das Prinzip ist jedoch "ubertragbar.
Wir stellen die "Uberg"ange zusammen, die im Zustand $R$ anzuf"ugen
sind. F"ur die Verarbeitung der Terminalsymbole brauchen wir
f"ur jedes Terminalsymbol einen "Ubergang, der gleichzeitig
ein Symbol vom Input wie vom Stack entfernt:
\begin{equation}
\entrymodifiers={++[o][F]}
\xymatrix{
{R}\ar@(ur,dr)^{{\tt 0},{\tt 0}\to\varepsilon}
   \ar@(ul,dl)_{{\tt 1},{\tt 1}\to\varepsilon}
}
\label{beispiel-tregel}
\end{equation}
Die Regel $S\to\varepsilon$ wird "ubergef"uhrt in einen "Ubergang
\begin{equation}
\entrymodifiers={++[o][F]}
\xymatrix{
{R}\ar@(ur,dr)^{\varepsilon, S\to\varepsilon}
}
\label{beispiel-epsilon}
\end{equation}
Die Regel $S\to {\tt 0}S{\tt 1}$ muss "ubergef"uhrt werden
in die drei Schritte
\begin{equation}
\entrymodifiers={++[o][F]}
\xymatrix{
{R}\ar[rr]^{\varepsilon,S\to{\tt 1}}
	&*+\txt{}
		&{q_1}\ar[dl]^{\varepsilon,\varepsilon\to S}
\\
*+\txt{}
	&{q_2}\ar[ul]^{\varepsilon,\varepsilon\to{\tt 0}}
}
\label{beispiel-0s1regel}
\end{equation}
Jetzt kann man den Ablauf des Akzeptierens des Wortes {\tt 0011}
verfolgen. Offenbar ist die Ableitung 
\[
S
\overset{\text{(\ref{beispiel-0s1regel})}}{\longrightarrow}
{\tt 0}S{\tt 1}
\overset{\text{(\ref{beispiel-0s1regel})}}{\longrightarrow}
{\tt 00}S{\tt 11}
\overset{\text{(\ref{beispiel-epsilon})}}{\longrightarrow}
{\tt 0011}
\]
Die Tabellen~\ref{beispiel-tabelle} zeigt den Inhalt des 
Input und des Stack nach jedem "Ubergang des Stackautomaten.
\begin{table}
\begin{center}
\begin{tabular}{|c|l|c|l|}
\hline
"Ubergang&Input&Zustand&Stack\\
\hline
                         &{\tt 0011}&$q_0$&\\
                         &{\tt 0011}&$S$  &${\tt \$}$\\
                         &{\tt 0011}&$R$  &$S\quad {\tt \$}$\\
(\ref{beispiel-0s1regel})&{\tt 0011}&$q_1$&{\tt 1}\\
(\ref{beispiel-0s1regel})&          &$q_2$&$S\quad {\tt 1}\quad{\tt \$}$\\
(\ref{beispiel-0s1regel})&          &$R$  &${\tt 0}\quad S\quad {\tt 1}\quad{\tt \$}$\\
(\ref{beispiel-tregel})  &{\tt 011} &$R$  &$S\quad {\tt 1}\quad{\tt \$}$\\
(\ref{beispiel-0s1regel})&{\tt 011}&$q_1$&${\tt 1}\quad {\tt 1}\quad{\tt \$}$\\
(\ref{beispiel-0s1regel})&          &$q_2$&$S\quad {\tt 1}\quad {\tt 1}\quad{\tt \$}$\\
(\ref{beispiel-0s1regel})&          &$R$  &${\tt 0}\quad S\quad {\tt 1}\quad {\tt 1}\quad{\tt \$}$\\
(\ref{beispiel-tregel})  &{\tt 11}  &$R$  &$S\quad {\tt 1}\quad {\tt 1}\quad{\tt \$}$\\
(\ref{beispiel-epsilon}) &{\tt 11}  &$R$  &${\tt 1}\quad {\tt 1}\quad{\tt \$}$\\
(\ref{beispiel-tregel})  &{\tt 1}   &$R$  &${\tt 1}\quad{\tt \$}$\\
(\ref{beispiel-tregel})  &{\tt }    &$R$  &${\tt \$}$\\
                         &{\tt }    &$A$  &\\
\hline
\end{tabular}
\end{center}
\caption{Ablauf des Akzeptierens des Wortes {\tt 0011} mit
Hilfe des Stackautomaten zur Grammatik $S\to {\tt 0}S{\tt 1}\;|\;\varepsilon$
\label{beispiel-tabelle}}
\end{table}
\end{beispiel}

\begin{hilfssatz}\label{pda_has_grammar}
Ist $P$ ein Stackautomat, dann gibt es eine Grammatik $G$, die die
gleiche Sprache produziert: $L(P)=L(G)$.
\end{hilfssatz}

\begin{proof}[Beweis]
Wir m"ussen eine Grammatik konstruieren, die die gleiche Sprache
erzeugt. Der Stackautomat sollte m"oglichst nahe an dem in
Hilfssatz \ref{hilfssatz_cfg_to_pushdown} konstruierten liegen, damit es
m"oglichst einfach wird, sich von der "Aquivalenz zu "uberzeugen.
Wir f"uhren daher an $P$ zun"achst drei einfache Modifikationen
durch: 
\begin{enumerate}
\item $P$ hat nur einen einzigen Akzeptierzustand
$q_a$.
Dies erreicht man dadurch, dass man alle Akzeptierzust"ande "uber
einen $\varepsilon$-"Ubergang mit
$q_a$ verbindet, und bisherigen Akzeptierzust"ande
werden dann zu gew"ohnlichen Zust"anden degradiert:
\[
\xymatrix{
*++[o][F=]{q}
	&\Rightarrow
		&*++[o][F]{q}\ar[r]^{\varepsilon,\varepsilon\to\varepsilon}
			&*++[o][F=]{q_a}
}		
\]
Der modifizierte Automat akzeptiert die gleichen W"orter wie $P$.
\item Vor dem Akzeptieren wird der Stack geleert. Dazu wird ein zus"atzliches
Zeichen ben"otigt, welches das Ende des Stacks signalisiert. Es wird zu
beginn auf den Stack geschrieben, und als letzter "Ubergang wieder
vom Stack genommen. Der Startzustand $q_0$ wird also ersetzt durch einen neuen
Startzustand $q_0'$, mit dem einzigen "Ubergang 
\[
\entrymodifiers={++[o][F]}
\xymatrix{
*+\txt{}\ar[r]
	&{q_0'}\ar[r]^{\varepsilon,\varepsilon\to {\tt\$}}
		&q_0
}
\]
und der Akzeptierzustand  durch einen neuen Akzeptierzustand $q_a'$ und
den "Ubergang
\[
\entrymodifiers={++[o][F]}
\xymatrix{
{q_a}\ar[r]^{\varepsilon,{\tt\$}\to\varepsilon}
	&*++[o][F=]{q_a'}
}
\]
\item Jeder "Ubergang legt ein Zeichen auf den Stack, oder entfernt
eines, aber er macht nicht beides gleichzeitig. Dazu muss jeder "Ubergang,
der gleichzeitig ein Zeichen vom Stack nimmt und ein neues dort ablegt
mit Hilfe eines neuen Zustandes in zwei "Uberg"ange aufgeteilt werden:
\[
\xymatrix{
*++[o][F]{p}\ar[dd]^{a,b\to c}
	&*+\txt{}
		&*++[o][F]{p}\ar[d]^{a,b\to\varepsilon}
\\
	&\Rightarrow
		&*++[o][F]{s}\ar[d]^{\varepsilon,\varepsilon\to c}
\\
*++[o][F]{q}
	&
		&*++[o][F]{q}
}
\]
Ausserdem m"ussen "Uberg"ange, die gar nicht auf dem Stack operieren,
durch "Uberg"ange ersetzt werden, die ein beliebiges Stacksymbol
schreiben und gleich wieder entfernen:
\[
\xymatrix{
*++[o][F]{p}\ar[dd]^{a,\varepsilon\to \varepsilon}
	&*+\txt{}
		&*++[o][F]{p}\ar[d]^{a,\varepsilon\to x}
\\
	&\Rightarrow
		&*++[o][F]{s}\ar[d]^{\varepsilon,x\to \varepsilon}
\\
*++[o][F]{q}
	&
		&*++[o][F]{q}
}
\]
\end{enumerate}
Damit k"onnen wir jetzt zur Konstruktion der Grammatik schreiten.
Die Idee dabei ist, Variablen $A_{pq}$ zu verwenden, die alle 
W"orter erzeugen, die durch Zustands"uberg"ange erzeugt werden,
die den Automaten vom Zustand $p$ in den Zustand $q$ bringen,
und den Stack im gleichen Zustand zur"ucklassen, den sie vorgefunden
haben.
Wir setzen $A_{pq}$ mit der Menge der W"orter gleich, die
auf diese Weise erzeugt werden k"onnen.

Dazu m"ussen wir verstehen, wie die W"orter entstehen, die von $P$
akzeptiert werden. W"ahrend der Berechnung, die zum Akzeptieren des
Wortes $w$ f"uhrt, wird zun"achst
irgend ein Zeichen $x$ auf den Stack gelegt, und am Ende muss ein
Zeichen entfernt werden, welches $x$ oder auch etwas anderes sein kann.

Falls das am Ende entfernte Zeichen $x$ ist, k"onnen wir dies durch
die Regel $A_{pq}\to aA_{rs}b$ symbolisieren, wobei $a$ das Zeichen
ist, welches zusammen mit dem Ablegen von $x$ vom Input verarbeitet wird,
und $b$ das Zeichen, das mit dem Entfernen von $x$ verarbeitet wird.

Falls das am Schluss vom Stack entfernte Zeichen nicht das Gleiche ist,
dann muss das $x$ irgendwann im Laufe der Berechnung entfernt worden
sein, und das neue Zeichen $y$ muss auf dem Stack abgelegt worden
sein. Es gibt also einen Zwischenzustand $r$, in dem der Stack
wieder im selben Zustand wie zu
Beginn der Berechnung ist, wir k"onnen dies durch
$A_{pq}\to A_{pr}A_{rq}$ symbolisieren.

Formal konstruieren wir also eine Grammatik mit Variablen
$V=\{A_{pq}\;|\; p,q\in Q\}$, Startvariable ist $A_{q_0,q_a}$.
Die Menge der Regeln bauen wir wie folgt auf:
\begin{itemize}
\item F"ur $p,q,r,s\in Q$, $t\in\Gamma$ und $a,b\in\Sigma_{\varepsilon}$:
Falls $\delta(p,a,\varepsilon)$ das Paar $(r,t)$ enth"alt
und $\delta(s,b,t)$ das Paar $(q,\varepsilon)$, f"uge die Regel
$A_{pq}\to aA_{rs}b$ hinzu.

Diese Regel besagt, dass W"orter zwischen $p$ und $q$ dadurch gebildet
werden k"onnen, dass zun"achst ein Zeichen $a$ verarbeitet, und
ein Zeichen $t$ auf den Stack geschrieben wird, dann wird ein Wort
in $A_{rs}$ erzeugt, und zum Schluss das Zeichen $t$ unter
gleichzeitiger Verarbeitung des Zeichens $b$ wieder vom Stack
genommen.

\[
\begin{gathered}

\entrymodifiers={++[o][F]}
\xymatrix{
{p}\ar[d]\ar[r]^{a,\varepsilon\to t}
	&{r}\ar[d]
\\
{q}
	&{s}\ar[l]^{b,t\to\varepsilon}
}
\end{gathered}

\qquad\rightsquigarrow\qquad A_{pq}\to aA_{rs}b
\]

\item F"ur drei Zust"ande $p,q,r\in Q$ f"uge die Regel 
$A_{pq}\to A_{pr}A_{rq}$ hinzu.
\[
\begin{gathered}
\entrymodifiers={++[o][F]}
\xymatrix{
{p}\ar[dr]\ar[dd]
\\
*+\txt{}
	&{r}\ar[dl]
\\
{q}
}
\end{gathered}
\qquad\rightsquigarrow\qquad A_{pq}\to A_{qr}A_{rq}
\]
\item F"ur jeden Zustand $p\in Q$ f"uge die Regel $A_{pp}\to \varepsilon$
hinzu:
\[
\begin{gathered}
\entrymodifiers={++[o][F]}
\xymatrix{
*+\txt{}
	&{p}
}
\end{gathered}
\qquad\rightsquigarrow\qquad
A_{pp}\to\varepsilon.
\]
\end{itemize}
Jetzt muss man nur noch zeigen, dass ein Wort $w$ genau dann aus $A_{pq}$
abgeleitet werden kann, wenn es $P$ vom Zustand $p$ mit leerem Stack
in den Zustand $q$ mit leerem Stack bringen kann.

\begin{hilfssatz}\label{apq_generates_x_implies}
Falls $A_{pq}$ das Wort $x$ erzeugt, dann kann $x$ $P$ aus dem Zustand
$p$ mit leerem Stack in den Zustand $q$ mit leerem Stack "uberf"uhren.
\end{hilfssatz}

\begin{proof}[Beweis von Hilfssatz \ref{apq_generates_x_implies}]
Man kann die vollst"andige Induktion f"ur die L"ange der be\-n"otigten 
Ableitung $A_{pq}\overset{*}{\Rightarrow} x$ verwenden.

Falls die Ableitung mit nur einem Schritt m"oglich ist, dann muss
dazu eine Regel der Form $A_{pp}\to\varepsilon$ verwendet werden,
denn dies sind die einzigen Regeln, die auf der rechten Seite
keine Variablen enthalten. 

Nehmen wir also an, f"ur Ableitungen der L"ange $k$ sei bereits
bekannt, dass sie den Automaten von einem Zustand mit leerem Stack
in einen anderen Zustand mit leerem Stack "uberf"uhren.

Sei jetzt $A_{pq}\overset{*}{\Rightarrow}x$ eine Ableitung mit $k+1$
Schritten. Der erste Schritt dieser Ableitung ist entweder eine
Regel der Form $A_{pq}\to aA_{rs}b$ oder $A_{pq}\to A_{pr}A_{rq}$.
Im ersten Fall sagt die Regel, dass $x=ayb$, wobei $y$ ein
Wort ist, welches aus $A_{rs}$ in h"ochstens $k$ Schritten abgeleitet
werden kann. Also kann nach Induktionsannahme $y$ den Automaten vom
Zustand $r$ mit leerem Stack in den Zustand $s$ mit leerem Stack
"uberf"uhren. Der "Ubergang von $q$ nach $r$ mit Inputzeichen $a$
legt m"oglicherweise ein Zeichen $t$ auf den Stack, nach Konstruktion
der Produktionsregeln wird dieses vom "Ubergang mit Inputzeichen $b$
auch wieder entfernt, so dass der Stack wieder leer ist.

Im zweiten Falls ist nach Induktionsannahme $x=yz$, wobei $y$ den
Automaten vom Zustand $q$ in den Zustand $r$ je mit leerem Stack
"uberf"uhrt, und $z$ ihn von $r$ nach $q$ je mit leerem Stack
f"uhrt.

In beiden F"allen folgt, dass $x$ den Automaten von $p$
nach $q$ je mit leerem Stack f"uhren kann.
Damit ist der Induktionsschritt vollzogen, und es folgt, dass
sich jede Ableitung durch "Uberg"ange im Stackautomaten zwischen
Zust"anden mit jeweils leerem Stack bilden lassen.
\end{proof}

\begin{hilfssatz}\label{implies_apq_generates_x}
Falls $x$ $P$ aus dem Zustand $p$ mit leerem Stack in den Zustand
$q$ mit leerem Stack "uberf"uhren kann, dann ist
$A_{pq}\overset{*}{\Rightarrow} x$.
\end{hilfssatz}

\begin{proof}[Beweis von Hilfssatz \ref{implies_apq_generates_x}]
Auch diesen Teil kann man mit vollst"andiger Induktion beweisen,
diesmal "uber die L"ange der Berechnung. 

Die k"urzeste m"ogliche Berechnung hat $0$ Schritte, d.\,h.~sie endet
im gleichen Zustand, in dem sie begonnen hat.
Und tats"achlich enth"alt die Grammatik die Regel $A_{pp}\to\varepsilon$, 
so dass das leere Wort tats"achlich aus $A_{pp}$ mit leerem
Stack abgeleitet werden kann.

Nehmen wir jetzt an, dass bereits bekannt ist, dass Berechnungen mit
$k$ Schritten, welche $P$ mit dem Input-Wort $w$ vom Zustand $p$
in den Zustand $q$ je mit leerem Stack dazu f"uhren dass
$A_{pq}\overset{*}{\Rightarrow} x$.

Sei jetzt also eine Berechnung mit Inputwort $x$
mit $k+1$ Schritten gegeben, die $P$ vom Zustand $p$ in den Zustand $q$ 
je mit leerem Stack f"uhrt. Entweder ist der Stack nur ganz zu
Beginn oder ganz am Schluss leer, oder er wird dazwischen einmal
leer.

Im ersten Fall wird ein Symbol $t$ beim ersten Schritt auf den Stack
gelegt, und beim letzten Schritt entfernt. Es gibt also zwei Zust"ande
$r$ und $s$ und "Uberg"ange
\[
\entrymodifiers={++[o][F]}
\xymatrix{
p\ar[r]^{a,\varepsilon\to t}
	&r\ar[r]
		&s\ar[r]^{b,t\to\varepsilon}
			&q
}
\]
Die Berechnung, die $P$ von $r$ in $s$ "uberf"uhrt, ist k"urzer, und kann
mit leerem Stack durchgef"uhrt werden.
Also ist sie nach Induktionsannahme der Teil $y$ in $x=ayb$ aus ableitbar 
$A_{rs}$ ableitbar.

Im zweiten Fall zerf"allt die Berechnung, in der $x$ $P$ von $p$ in
$q$  mit leerem Stack "uberf"uhrt in zwei Teile, die mit Inputw"ortern
$y$ und $z$ $p$ in $r$ bzw.~$r$ in $q$ je mit leerem Stack "uberf"uhren:
\[
\entrymodifiers={++[o][F]}
\xymatrix{
p\ar[r]
	&r\ar[r]
		&q
}
\]
Nach Induktionsannahme ist daher
$A_{pr}\overset{*}{\Rightarrow}y$
$A_{rq}\overset{*}{\Rightarrow}z$, und zusammen mit der Regel
$A_{pq}\to A_{pr}A_{rq}$ der Grammatik auch
$A_{pq}\overset{*}{\Rightarrow} x$
\end{proof}

Damit ist auch der Beweis von Hilfssatz \ref{pda_has_grammar} vollst"andig.
\end{proof}

\section{Nicht kontextfreie Sprachen}
\rhead{Nicht kontextfreie Sprachen}
Mit einem Stack kann man nur "uber eine Art von eingegangenen
Zeichen Buch f"uhren. M"ochte man mehrere Zeichen verfolgen,
br"auchte man freien Zugriff auf verschiedene Z"ahler f"ur jedes
Zeichen. Eine Maschine mit so vielen Stacks wie Zeichen zur Verf"ugung
stehen, k"onnte also auch die Sprache
\[
L=\{ {\tt a}^n {\tt b}^n {\tt c}^n\;|\;n\in\mathbb N\}
\]
erkennen. Ein ``standard'' Stackautomat kann das jedoch nicht,
in diesem Abschnitt soll gezeigt werden, dass $L$ nicht kontextfrei
ist. 

\subsection{Pumping Lemma f"ur kontextfreie Sprachen}
\index{Pumping Lemma!f\"ur kontextfreie Sprachen}
Bei regul"aren Sprachen erm"oglicht das Pumping-Lemma den einfachen
Beweis, dass eine Sprache nicht regul"ar ist. Die Grundidee dabei ist,
dass sich in einem endlichen Automaten fr"uher oder sp"ater ein
Zustand wiederholen muss, und dass die Schleife zwischen dem ersten
und dem zweiten Auftreten dieses Zustandes beliebig oft durchlaufen
werden kann, um immer wieder neue akzeptable W"orter zu liefern.

Die "Aquivalenz von kontextfreien Sprachen mit Sprachen, die von einem
Stackautomaten akzeptiert werden k"onnen, suggeriert, dass so etwas
auch f"ur kontextfreie Sprachen m"oglich sein sollte. Stackautomaten
wurden aber grunds"atzlich als nicht deterministische Automaten
konstruiert, wo die Argumente, die das Pumping Lemma f"ur regul"are
Sprachen geliefert haben, nicht direkt anwendbar sind. Wir verwenden
daher nicht einen Stackautomaten, sondern direkt die Grammatik, um
das Pumping Lemma herzuleiten.

\begin{figure}
\begin{center}
\includegraphics[width=0.5\hsize]{images/cfg-1}
\end{center}
\caption{Parse Tree f"ur die Erzeugung des Wortes $uvxyz$ aus der
Startvariablen $S$.\label{cfg-tree-1}}
\end{figure}
\begin{figure}
\begin{center}
\begin{tabular}{cc}
\includegraphics[width=0.45\hsize]{images/cfg-2}&%
\includegraphics[width=0.45\hsize]{images/cfg-3}\\
\end{tabular}
\end{center}
\caption{Parse Tree f"ur das aufgepumpte Wort $uv^2xy^2zv$ (links) und das
abgepumpte Wort $uxz$ (rechts).\label{cfg-tree-2}}
\end{figure}

Ist ein Wort $w\in L(G)$ gen"ugen lang, gibt es auch lange Pfade im
Ableitungsbaum. Bei Verwendung der Chomsky-Normalform ist die 
Tiefe des Baumes im besten Fall $\log_2 |w|$, im schlimmsten Fall $|w|-1$.
Ist $|w|>2^{|V|}$, muss mindestens eine
Variable auf einem Ast des Baumes zweimal verwenden\footnote{
Innerhalb des Baumes werden genau $|w|-1$ Regeln der
Form $A\to BC$ angewendet.
Trotzdem reicht es nicht, $|w|-1>|V|$ zu verlangen, weil die
zweimal verwendete Variable nicht auf dem gleichen Ast des
Baumes zu sein braucht. F"ur die Durchf"uhrung des Argumentes
brauchen wir aber, dass wir die beiden Vorkommnisse der Variablen
"uber Ableitungsregeln verbinden k"onnen.}.
Sei $A$ die unterste Variable im Ableitungsbaum, die wiederholt
wird (Abbildung~\ref{cfg-tree-1}).
Es erzeugt ein Wort $x$. Das n"achsth"ohere Vorkommen von $A$
erzeugt dagegen ein Wort, welches aus drei Teilen besteht:
$vxy$, die L"ange dieses Wortes ist $\le N$. F"ur das ganze Wort fehlen
noch die Teile, die von Regeln ``weiter oben'' im Ableitungsbaum
erzeugt werden, also $w=uvxyz$.
Da die beiden Vorkommnisse von $A$ veschieden sind, muss mindestens
eines der Teilw"orter $v$ und $y$ nicht leer sein, also $|vy|>0$
Indem man den Teil des Ableitungsbaumes
zwischen den Vorkommnissen von $A$ repliziert, kann man jetzt die
W"orter $uv^kxy^kz$ bilden, die alle auch mit der Grammatik $G$ 
abgeleitet werden k"onnen, also zu $L(G)$ geh"oren (Abbildung~\ref{cfg-tree-2}).
Damit haben wir folgenden Satz bewiesen:

\begin{satz}[Pumping Lemma f"ur kontextfreie Sprachen]
\index{Pumping Lemma!f\"ur kontextfreie Sprachen}
\index{pumping length!f\"ur kontextfreie Sprachen}
Sei $L$ eine kontextfreie Sprache, dann gibt es ein Zahl $N$, die pumping
length, so dass jedes Wort $w\in L$ mit $|w|\ge N$ zerlegt werden
kann in f"unf Teile $w=uvxyz$
\begin{compactenum}
\item
$|vy|>0$,
\item
$|vxy|\le N$ und
\item
$uv^kxy^kz\in L$ f"ur alle $k\in\mathbb N$.
\end{compactenum}
\end{satz}

\subsection{Beispiele nicht kontextfreier Sprachen}
\subsubsection{Die Sprache $L=\{a^nb^nc^n\,|\,n\in\mathbb N\}$.}
\begin{figure}
\begin{center}
\includegraphics[width=\hsize]{images/pl-5}\\%
\smallskip
\includegraphics[width=\hsize]{images/pl-6}\\%
\smallskip
\includegraphics[width=\hsize]{images/pl-7}\\%
\smallskip
\includegraphics[width=\hsize]{images/pl-8}%
\end{center}
\caption{Pumping Lemma f"ur kontextfreie Sprachen angewandt auf das 
Wort ${\tt a}^N{\tt b}^N{\tt c }^N\in L$, wobei $N$ die pumping length
von $L$ ist. Da $w$ lang genug ist, gibt es eine Zerlegung 
$w=uvxyz$ (2.~Zeile).
Abpumpen (3.~Zeile) und Aufpumpen (4.~Zeile) des
Wortes f"uhrt zu W"ortern, die nicht mehr in $L$ liegen.\label{pumpingcfgimage}}
\end{figure}

Die Sprache $L=\{a^nb^nc^n\;|\;n\in\mathbb N\}$ ist nicht kontextfrei.
Wir verwenden das Pumping Lemma f"ur kontextfreie Sprachen.  Dazu
nehmen wir zun"achst an, die Sprache $L$ sei kontextfrei. Dann
besagt das Pumping Lemma, dass es eine Zahl $N$ gibt, so dass
W"orter mit L"ange mindestens $N$ besondere Eigenschaften haben.
Als solches langes Wort nehmen wir $w=a^Nb^Nc^N$. Nach dem
Pumping Lemma gibt es eine Zerlegung in f"unf Teile
$w=uvxyz$, wobei der mittlere Teile nicht zu lang ist:
$|vxy|\le N$ (Abbildung~\ref{pumpingcfgimage}).
Insbesondere enth"alt $|vxy|$ h"ochstens zwei
Arten von Zeichen, denn es ist zu kurz, die $N$ $b$s in der Mitte
von $w$ zu "uberspannen. Beim Aufpumpen zu $uv^kxy^kz$ nimmt also
die Zahl dieser beiden Zeichen zu, nicht jedoch die Zahl des nicht
in $vxy$ enthaltenen Zeichens.
Damit kann $uv^kxy^kz$ nicht mehr in $L$ sein, obwohl das
Pumping Lemma dies behauptet. Aus diesem Widerspruch folgt,
dass $L$ nicht kontextfrei sein kann.

\rhead{Praktische Parser}
\input realworldparser.tex
